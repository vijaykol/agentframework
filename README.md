# Advanced Customer Support Agent using Microsoft Agent Framework

customer support agent built with **Microsoft Agent Framework** and **Azure AI Foundry**, featuring comprehensive middleware layers, custom AI tools, and a beautiful web-based UI powered by **DevUI**.

## ğŸ¯ Overview

This project demonstrates a complete implementation of the Microsoft Agent Framework with:

- **Native Framework Components**: `@chat_middleware`, `@ai_function`, `ChatContext`, `ChatMessage`, `TextContent`
- **Azure AI Foundry Integration**: Powered by gpt-4o deployment
- **Multiple Middleware Layers**: Logging, user context, validation, and analytics
- **Custom AI Tools**: User lookup, ticket search, ticket creation, service status
- **DevUI Web Interface**: Interactive chat and monitoring dashboard
- **Async/Await Support**: Full async implementation with sync-to-async wrapper
- **Extensive Logging**: Comprehensive logging throughout the system

## âœ¨ Features

### Core Agent Framework Features

| Feature | Description |
|---------|-------------|
| **@chat_middleware** | 4 middleware layers that process messages sequentially (logging â†’ context â†’ validation â†’ analytics) |
| **@ai_function** | 4 custom tools that the LLM can call automatically based on user intent |
| **ChatContext** | Container for entire conversation with list of ChatMessages |
| **ChatMessage** | Individual messages with TextContent, metadata, and role (user/assistant) |
| **TextContent** | Structured text wrapper with language, encoding, and metadata support |

### Middleware Pipeline

1. **Logging Middleware** - Logs all incoming messages
2. **User Context Middleware** - Enriches messages with user data
3. **Validation Middleware** - Validates message quality and format
4. **Analytics Middleware** - Tracks metrics and performance

### AI Tools (Functions)

The agent can automatically call these tools based on user intent:

```python
@ai_function
def get_user_info(user_id: str) -> str:
    """Get user information from the database"""

@ai_function
def search_tickets(user_id: str) -> str:
    """Search support tickets for a user"""

@ai_function
def create_ticket(user_id: str, subject: str, priority: str) -> str:
    """Create a new support ticket"""

@ai_function
def get_service_status() -> str:
    """Get current service status"""
```

### How Tool Calling Works

```
User Message
    â†“
LLM Analyzes Intent (automatically)
    â†“
LLM Decides Which Tools to Call (based on tool schemas)
    â†“
Framework Executes Tools (via JSON)
    â†“
LLM Synthesizes Response
    â†“
Response to User
```

The LLM receives JSON schemas of all available tools and automatically decides which ones to call based on the user's message. No explicit if/else statements needed!

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Microsoft Agent Framework
- Azure AI Foundry access
- Azure credentials configured



### Configuration

Set your Azure credentials and Foundry endpoint:

```bash

$env:FOUNDRY_ENDPOINT = ""
$env:FOUNDRY_DEPLOYMENT = "gpt-4o"


```

### Running with DevUI



## ğŸ¨ DevUI - Web Interface

DevUI provides a beautiful web-based interface to interact with your agent:

### Features

- **Interactive Chat** - Chat with the agent in real-time
- **Conversation History** - View full conversation history
- **Middleware Monitoring** - Watch each middleware layer execute
- **Tool Execution Trace** - See which tools were called and their results
- **Tool Inspector** - View all available tools and their parameters
- **Events Dashboard** - Real-time event tracking
- **Traces** - Detailed execution traces
- **Performance Metrics** - Track response times and resource usage

### Example Interactions

```
User: "URGENT! I was charged twice!"
â†’ DevUI shows: middleware execution â†’ get_user_info() â†’ search_tickets() â†’ create_ticket() â†’ response

User: "What is my account status?"
â†’ DevUI shows: get_user_info() tool call â†’ user data â†’ response

User: "Check service status"
â†’ DevUI shows: get_service_status() tool call â†’ status info â†’ response
```

## ğŸ“Š Architecture

### Data Flow

```
User Input (DevUI Chat)
    â†“
ChatMessage Created (with TextContent)
    â†“
Middleware Pipeline
  â”œâ”€ Logging Middleware
  â”œâ”€ User Context Middleware
  â”œâ”€ Validation Middleware
  â””â”€ Analytics Middleware
    â†“
LLM Processing (with tool schemas)
    â†“
Tool Execution (if needed)
    â†“
Response Synthesis
    â†“
DevUI Display
```

### ChatContext & ChatMessage Relationship

```
ChatContext (entire conversation container)
  â””â”€ messages: List[ChatMessage]
      â”œâ”€ Message 1: User asks something
      â”‚   â””â”€ content: TextContent
      â”œâ”€ Message 2: Agent responds
      â”‚   â””â”€ content: TextContent
      â”œâ”€ Message 3: User follows up
      â”‚   â””â”€ content: TextContent
      â””â”€ Message 4: Agent responds again
          â””â”€ content: TextContent
```

Each turn adds new ChatMessages to the list. The LLM always receives the entire list for full context.

## ğŸ”§ Code Structure

```
advanced_agent_demo_with_devui_final.py
â”œâ”€â”€ Database Simulation (Users & Tickets)
â”œâ”€â”€ Async Wrapper (for sync OpenAI client)
â”œâ”€â”€ AI Functions (@ai_function)
â”‚   â”œâ”€â”€ get_user_info
â”‚   â”œâ”€â”€ search_tickets
â”‚   â”œâ”€â”€ create_ticket
â”‚   â””â”€â”€ get_service_status
â”œâ”€â”€ Middleware Layers (@chat_middleware)
â”‚   â”œâ”€â”€ logging_middleware
â”‚   â”œâ”€â”€ user_context_middleware
â”‚   â”œâ”€â”€ validation_middleware
â”‚   â””â”€â”€ analytics_middleware
â”œâ”€â”€ Foundry Client Setup
â”œâ”€â”€ Agent Creation
â””â”€â”€ Main (DevUI Server)
```

## ğŸ’¡ Key Concepts

### Intent Determination

The LLM automatically determines user intent by analyzing the message. It doesn't need explicit programming:

```python
# No if/else needed! LLM figures it out:
"URGENT! I was charged twice!" 
â†’ LLM intent: billing issue, needs user info and ticket creation
â†’ Calls: get_user_info() â†’ search_tickets() â†’ create_ticket()

"What's my status?"
â†’ LLM intent: account inquiry
â†’ Calls: get_user_info()

"Is the service working?"
â†’ LLM intent: status check
â†’ Calls: get_service_status()
```

### Tool Calling Flow

1. **LLM Receives Tool Schemas** - JSON descriptions of all available tools
2. **LLM Decides** - Based on user message, which tools to call
3. **Framework Executes** - Parses JSON, calls Python functions
4. **Results Returned** - Tool results sent back to LLM
5. **LLM Synthesizes** - Creates natural response using tool results

### Good Practice: Docstrings

Always provide clear docstrings for your tools:

```python
@ai_function
def get_user_info(user_id: Annotated[str, "The user ID to look up"]) -> str:
    """Get user information from the database"""
    # The docstring and parameter annotations help the LLM understand what this tool does
```

## ğŸ“ Middleware Deep Dive

Middleware runs sequentially and can:
- **Process** messages before/after LLM
- **Enrich** messages with additional data
- **Validate** message quality
- **Monitor** and track metrics

```python
@chat_middleware
async def logging_middleware(context: ChatContext) -> None:
    """Middleware 1: Logging all messages"""
    if context.messages:
        msg_preview = context.messages[-1].content[:50]
        logger.info(f"Message: {msg_preview}...")
```

## ğŸ§ª Testing

The project includes 4 test scenarios (can be run without DevUI):

```bash
# Run test scenarios (console only)
python advanced_agent_demo.py
```

Test scenarios:
1. **Urgent Billing Issue** - Tests multiple tool calls
2. **Technical Support** - Tests API error handling
3. **General Inquiry** - Tests service status tool
4. **Multi-Turn Conversation** - Tests conversation history

## ğŸ“š Understanding the Framework

### How @chat_middleware Works

Middleware intercepts ChatContext before/after LLM processing:

```
Input â†’ MW1 â†’ MW2 â†’ MW3 â†’ MW4 â†’ LLM â†’ Response
```

Each middleware can:
- Read messages from context
- Add metadata
- Validate content
- Track analytics

### How @ai_function Works

Functions decorated with `@ai_function` are automatically:
1. Converted to JSON schemas
2. Sent to LLM as available tools
3. Called by framework when LLM decides
4. Results returned to LLM

### How ChatContext Flows

ChatContext is passed through the entire pipeline:
- Created when user sends message
- Passed through all middleware
- Sent to LLM with full history
- Updated with new messages
- Returned to user

## ğŸ” Security Considerations

- **Credentials**: Use Azure Identity for secure authentication
- **Logging**: Be careful not to log sensitive user data
- **Tool Access**: Validate user permissions before executing tools
- **Input Validation**: Always validate user input in middleware

## ğŸ› Troubleshooting

### 404 Error from Foundry

If you get a 404 error, check:
- Foundry endpoint is correct
- Model deployment name matches (gpt-4o)
- Azure credentials are valid
- Network connectivity to Foundry

### DevUI Not Opening

- Check if port 8080 is available
- Try `http://localhost:8080` manually
- Check console logs for errors

### Tool Not Being Called

- Verify tool has `@ai_function` decorator
- Check tool docstring is clear
- Ensure parameter annotations are present
- Test with explicit user message mentioning the tool

## ğŸ“– Additional Resources

- [Microsoft Agent Framework Documentation](https://learn.microsoft.com/en-us/agent-framework/overview/agent-framework-overview)


## ğŸ¤ Contributing

Feel free to extend this project with:
- Additional middleware layers
- More AI tools
- Custom database implementations
- Enhanced DevUI features

## ğŸ“„ License

MIT License - feel free to use this project as a reference or starting point.



**Built with â¤ï¸ using Microsoft Agent Framework and Azure AI Foundry**
